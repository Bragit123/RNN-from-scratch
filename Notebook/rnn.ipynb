{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network (RNN) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN), as opposed to a regular fully connected neural network (FCNN), has layers that are connected to themselves.\n",
    "\n",
    "The difference might be clearer by first looking at an FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fcnn.svg\" width=\"40%\" alt=\"FCNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an FCNN there are no connections between nodes in a single layer. For instance, $h_1^1$ is not connected to $h_2^1$. In addition, the input and output are always of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, however, this is no longer the case. Nodes in the hidden layers are connected to themselves, represented by the curved lines in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the output $\\vec{h}$ from the hidden layer is fed back into the hidden layer. This recurrence makes RNNs useful when working with sequential data, as we can have input of variable length. This is more clear if we unfold the recurrent part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_unfold.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematics of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some sequential input $X$ with $n$ features. Note that $X$ here is an array with two axes, since it contains $n$ features at each time step in the sequence. We will denote the input at a specific time step $t$ as\n",
    "$$\\vec{X}^{(t)} = \\begin{pmatrix}\n",
    "X^{(t)}_1 \\\\ \\vdots \\\\ X^{(t)}_n\n",
    "\\end{pmatrix},$$\n",
    "which is then an $n$-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an RNN with $L$ hidden layers, and an output layer with $m$ features. We will denote the output of the $l$'th hidden layer at time step $t$ as\n",
    "$$\\vec{h}_l^{(t)} = \\begin{pmatrix}\n",
    "h_{l, 1}^{(t)} \\\\ \\vdots \\\\ h_{l, n_l}^{(t)}\n",
    "\\end{pmatrix},$$\n",
    "with $n_l$ being the number of features in the $l$'th hidden layer. The output of the RNN at time step $t$ is denoted\n",
    "$$\\hat{\\vec{y}}^{(t)} = \\begin{pmatrix}\n",
    "\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_m,\n",
    "\\end{pmatrix}$$\n",
    "where the hat is there to distinguish the RNN output $\\hat{\\vec{y}}^{(t)}$ from the target value, which is denoted $\\vec{y}^{(t)}$.\n",
    "The RNN will then look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/large_rnn.svg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let $W_x$ and $\\vec{b}_x$ be the weight matrix and bias vector relating the input to the first hidden layer, and let $\\sigma_1$ be the activation function in the first hidden layer. The output from the first hidden layer at the first time step is then\n",
    "$$ \\vec{h}_1^{(1)} = \\sigma_1 \\left(W_x \\vec{X}^{(1)} + \\vec{b}_x \\right). $$\n",
    "To simplify our notation, we define\n",
    "$$ \\vec{z}_1^{(1)} = W_x \\vec{X}^{(1)} + \\vec{b}_x, $$\n",
    "such that\n",
    "$$ \\vec{h}_1^{(1)} = \\sigma_1 (\\vec{z}_1^{(1)}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At later time steps than the first, we will also need to consider the contribution from the previous time step. If we define $W_h^{11}$ and $\\vec{b}_h^{11}$ to be the weight matrix and bias vector relating the hidden node at time step $t-1$ to the node at time step $t$, both in the first hidden layer, we get the following equations for propagating forward through the first hidden layer.\n",
    "\\begin{align}\n",
    "\\vec{z}_1^{(t)} &= W_x \\vec{X}^{(t)} + \\vec{b}_x + W_h^{11} \\vec{h}_1^{(t-1)} + \\vec{b}_h^{11}\n",
    "\\\\\n",
    "\\vec{h}_1^{(t)} &= \\sigma_1 (\\vec{z}_1^{(t)}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see a pattern in the notation here. Let $W_h^{l_1 l_2}$ and $\\vec{b}_h^{l_1 l_2}$ be the weight matrix and bias vector relating nodes in the $l_1$'th layer to the $l_2$'th layer. If $l_1 = l_2-1$ we are looking at a connection between two nodes at the same time step, but in subsequent layers; and if $l_1 = l_2$ we are looking at a connection between two nodes in the same layer, but at subsequent time steps. Let $\\sigma_l$ be the activation function for nodes in the $l$'th hidden layer. We then have the following equations for forward propagation through the node.\n",
    "\\begin{align}\n",
    "\\vec{z}_l^{(t)} &= W_h^{l-1,l} \\vec{h}_{l-1}^{(t)} + \\vec{b}_h^{l-1,l} + W_h^{ll} \\vec{h}_l^{t-1} + \\vec{b}_h^{ll}\n",
    "\\\\\n",
    "\\vec{h}_l^{(t)} &= \\sigma_l (\\vec{z}_l^{(t)}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This expression is valid up to $l=L$, which is the last hidden layer. When feeding forward to the output layer we will no longer consider previous time steps. Let $W_y$ and $\\vec{b}_y$ denote the weight matrix and bias vector of the output layer, and let $\\sigma_y$ be the output activation function. We then get the following equations for forward propagation through the output layer.\n",
    "\\begin{align}\n",
    "\\vec{o}^{(t)} &= W_y \\vec{h}_L^{(t)} + \\vec{b}_y\n",
    "\\\\\n",
    "\\hat{\\vec{y}}^{(t)} &= \\sigma_y (\\vec{o}^{(t)}).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To simplify notation we will rename \n",
    "$$\n",
    "W_x \\to W^{01} \\\\\n",
    "\\vec{b}_x \\to \\vec{b}^{01} \\\\\n",
    "W_y \\to W^{L,L+1} \\\\\n",
    "\\vec{b}_y \\to \\vec{b}^{L,L+1} \\\\\n",
    "W_h^{l_1 l_2} \\to W^{l_1 l_2} \\\\\n",
    "\\vec{b}_h^{l_1 l_2} \\to \\vec{b}^{l_1 l_2}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to propagate forward through the network we need some weights and biases to connect the nodes. To simplify the notation going forward, we will consider the input layer to be the *zeroth layer*, and the output layer to be the *$L+1$'th layer*. We need each node to propagate to the node at the next layer (keeping the time step constant), and the next time step (keeping the layer constant), except for the input and output layers which do not connect to each other (as illustrated in the diagram above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W_{l,l+1}$ be the weight matrix and $\\vec{b}_{l,l+1}$ the bias vector, both connecting nodes at the $l$'th layer to the $l+1$'th layer, keeping the time step constant. Next, let $W_{ll}$ be the weight matrix and $\\vec{b}_{ll}$ the bias vector, both connecting nodes at subsequent time steps in the same layer. Also, let $\\sigma_l$ be the activation function in the $l$'th layer. Lastly, define the weighted sum $\\vec{z}_l^{(t)}$ at layer $l$ and time step $t$ such that the output of the node is the activation of that weighted sum, that is, such that $\\vec{h}_l^{(t)} = \\sigma_l (\\vec{z}_l^{(t)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these definitions the output from the first hidden layer at the first time step is then\n",
    "$$ \\vec{h}_1^{(1)} = \\sigma_1 \\left( \\vec{z}_1^{(1)} \\right), $$\n",
    "with\n",
    "$$ \\vec{z}_1^{(1)} = W_{01} \\vec{X}^{(1)} + \\vec{b}_{01}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At later time steps we will also need to consider the contribution from the previous time step. Hence for $t \\geq 2$ we will define\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} = W_{01} X^{(t)} + \\vec{b}_{01}$$\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{time} = W_{11} \\vec{h}_1^{(t-1)} + \\vec{b}_{11},$$\n",
    "such that $\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer}$ is the contribution from the previous layer, and $\\left( \\vec{z}_1^{(t)} \\right)_\\text{time}$ is the contribution from the previous time step. We then have\n",
    "$$\\vec{z}_1^{(t)} = \\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} + \\left( \\vec{z}_1^{(t)} \\right)_\\text{time},$$\n",
    "and\n",
    "$$\\vec{h}_1^{(t)} = \\sigma_1 \\left( \\vec{z}_1^{(t)} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression is exactly the same for any hidden node, but for $l \\geq 2$ we substitute $\\vec{X}^{(t)}$ with $\\vec{h}_{l-1}^{(t)}$. Thus for the $l$'th layer and $t$'th time step we have\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{layer} = W_{l-1,l} \\vec{h}_{l-1}^{(t)} + \\vec{b}_{l-1,l} $$\n",
    "and\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{time} = W_{ll} \\vec{h}_{l}^{(t-1)} + \\vec{b}_{ll}, $$\n",
    "that combine to give\n",
    "$$ \\vec{z}_l^{(t)} = \\left( \\vec{z}_l^{(t)} \\right)_{layer} + \\left( \\vec{z}_l^{(t)} \\right)_{time}, $$\n",
    "which in turn results in\n",
    "$$ \\vec{h}_l^{(t)} = \\sigma_l \\left( \\vec{z}_l^{(t)} \\right). $$\n",
    "This is also valid at the first time step by setting $\\left( \\vec{z}_l^{(1)} \\right)_\\text{time} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the output layer is exactly the same as above, but with $\\left( \\vec{z}_l^{(t)} \\right)_\\text{time} = 0$. Thus we have\n",
    "$$ \\vec{z}_{L+1}^{(t)} = \\left( \\vec{z}_{L+1}^{(t)} \\right)_\\text{layer} = W_{L,L+1} \\vec{h}_L^{(t)} + \\vec{b}_{L,L+1} $$\n",
    "and\n",
    "$$ \\hat{\\vec{y}}^{(t)} = \\sigma_{L+1} \\left( \\vec{z}_{L+1}^{(t)} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations given for the forward propagation can seem a bit messy, so it is nice to have a more visual aid of what is going on. Here is a diagram of the complete RNN including the weights and biases relating the different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward.svg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a weights and biases connected to a single arbitrary node. The green arrows represent input to the node, and the red arrows represent the output from the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward_node.svg\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation in an RNN works by comparing the output of the network to some target output (just as in the regular neural network), and propagating backwards through both the layers and the *time sequence*. It is therefore commonly referred to as *backpropagation through time* (BPTT). We will now derive the necessary equations to perform BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we have propagated forward through the network, and have produced some output $\\hat{\\vec{y}}^{(t)}$. We want to compare this with some target output value $\\vec{y}^{(t)}$, and will do so through a cost function $C (\\hat{\\vec{y}}, \\vec{y})$. Note that, although the cost *function* $C$ is the same at all time steps, the *value* it gives will be different at different time steps. We will denote the cost at time step $t$ by $C^{(t)} \\left(\\hat{\\vec{y}}^{(t)}, \\vec{y}^{(t)} \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cost function at each time step, we want to compute the gradient with respect to each weight and bias, that is, we want to compute\n",
    "$$\n",
    "\\frac{\\partial C^{(t)}}{\\partial W_x} \\; , \\; \\frac{\\partial C^{(t)}}{\\partial \\vec{b}_x} \\\\\n",
    "\\frac{\\partial C^{(t)}}{\\partial W_y} \\; , \\; \\frac{\\partial C^{(t)}}{\\partial \\vec{b}_y} \\\\\n",
    "\\frac{\\partial C^{(t)}}{\\partial W_h^{ij}} \\; , \\; \\frac{\\partial C^{(t)}}{\\partial \\vec{b}_h^{ij}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
