{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network (RNN) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN), as opposed to a regular fully connected neural network (FCNN), has layers that are connected to themselves.\n",
    "\n",
    "The difference might be clearer by first looking at an FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fcnn.svg\" width=\"720px\" alt=\"FCNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an FCNN there are no connections between nodes in a single layer. For instance, $h_1^1$ is not connected to $h_2^1$. In addition, the input and output are always of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, however, this is no longer the case. Nodes in the hidden layers are connected to themselves, represented by the curved lines in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn.svg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the output $\\vec{h}$ from the hidden layer is fed back into the hidden layer. This recurrence makes RNNs useful when working with sequential data, as we can have input of variable length. This is more clear if we unfold the recurrent part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_unfold.svg\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mathematics of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some sequential input $X$ with $n$ features. Note that $X$ here is an array with two axes, since it contains $n$ features at each time step in the sequence. We will denote the input at a specific time step $t$ as\n",
    "$$\\vec{X}^{(t)} = \\begin{pmatrix}\n",
    "X^{(t)}_1 \\\\ \\vdots \\\\ X^{(t)}_n\n",
    "\\end{pmatrix},$$\n",
    "which is then an $n$-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an RNN with $L$ hidden layers, and an output layer with $m$ features. We will denote the output of the $l$'th hidden layer at time step $t$ as\n",
    "$$\\vec{h}_l^{(t)} = \\begin{pmatrix}\n",
    "h_{l, 1}^{(t)} \\\\ \\vdots \\\\ h_{l, n_l}^{(t)}\n",
    "\\end{pmatrix},$$\n",
    "with $n_l$ being the number of features in the $l$'th hidden layer. The output of the RNN at time step $t$ is denoted\n",
    "$$\\hat{\\vec{y}}^{(t)} = \\begin{pmatrix}\n",
    "\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_m,\n",
    "\\end{pmatrix}$$\n",
    "where the hat is there to distinguish the RNN output $\\hat{\\vec{y}}^{(t)}$ from the target value, which is denoted $\\vec{y}^{(t)}$.\n",
    "The RNN will then look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/large_rnn.svg\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to propagate forward through the network we need some weights and biases to connect the nodes. To simplify the notation going forward, we will consider the input layer to be the *zeroth layer*, and the output layer to be the *$L+1$'th layer*. We need each node to propagate to the node at the next layer (keeping the time step constant), and the next time step (keeping the layer constant), except for the input and output layers which do not connect to each other (as illustrated in the diagram above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $W^{l,l+1}$ be the weight matrix and $\\vec{b}^{l,l+1}$ the bias vector, both connecting nodes at the $l$'th layer to the $l+1$'th layer, keeping the time step constant. Next, let $W^{ll}$ be the weight matrix and $\\vec{b}^{ll}$ the bias vector, both connecting nodes at subsequent time steps in the same layer. Also, let $\\sigma_l$ be the activation function in the $l$'th layer. Lastly, define the weighted sum $\\vec{z}_l^{(t)}$ at layer $l$ and time step $t$ such that the output of the node is the activation of that weighted sum, that is, such that $\\vec{h}_l^{(t)} = \\sigma_l (\\vec{z}_l^{(t)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these definitions the output from the first hidden layer at the first time step is then\n",
    "$$ \\vec{h}_1^{(1)} = \\sigma_1 \\left( \\vec{z}_1^{(1)} \\right), $$\n",
    "with\n",
    "$$ \\vec{z}_1^{(1)} = W^{01} \\vec{X}^{(1)} + \\vec{b}^{01}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At later time steps we will also need to consider the contribution from the previous time step. Hence for $t \\geq 2$ we will define\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} = W^{01} X^{(t)} + \\vec{b}^{01}$$\n",
    "$$\\left( \\vec{z}_1^{(t)} \\right)_\\text{time} = W^{11} \\vec{h}_1^{(t-1)} + \\vec{b}^{11},$$\n",
    "such that $\\left( \\vec{z}_1^{(t)} \\right)_\\text{layer}$ is the contribution from the previous layer, and $\\left( \\vec{z}_1^{(t)} \\right)_\\text{time}$ is the contribution from the previous time step. We then have\n",
    "$$\\vec{z}_1^{(t)} = \\left( \\vec{z}_1^{(t)} \\right)_\\text{layer} + \\left( \\vec{z}_1^{(t)} \\right)_\\text{time},$$\n",
    "and\n",
    "$$\\vec{h}_1^{(t)} = \\sigma_1 \\left( \\vec{z}_1^{(t)} \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression is exactly the same for any hidden node, but for $l \\geq 2$ we substitute $\\vec{X}^{(t)}$ with $\\vec{h}_{l-1}^{(t)}$. Thus for the $l$'th layer and $t$'th time step we have\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{layer} = W^{l-1,l} \\vec{h}_{l-1}^{(t)} + \\vec{b}^{l-1,l} $$\n",
    "and\n",
    "$$ \\left( \\vec{z}_l^{(t)} \\right)_{time} = W^{ll} \\vec{h}_{l}^{(t-1)} + \\vec{b}^{ll}, $$\n",
    "that combine to give\n",
    "$$ \\vec{z}_l^{(t)} = \\left( \\vec{z}_l^{(t)} \\right)_{layer} + \\left( \\vec{z}_l^{(t)} \\right)_{time}, $$\n",
    "which in turn results in\n",
    "$$ \\vec{h}_l^{(t)} = \\sigma_l \\left( \\vec{z}_l^{(t)} \\right). $$\n",
    "This is also valid at the first time step by setting $\\left( \\vec{z}_l^{(1)} \\right)_\\text{time} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression for the output layer is exactly the same as above, but with $\\left( \\vec{z}_l^{(t)} \\right)_\\text{time} = 0$. Thus we have\n",
    "$$ \\vec{z}_{L+1}^{(t)} = \\left( \\vec{z}_{L+1}^{(t)} \\right)_\\text{layer} = W^{L,L+1} \\vec{h}_L^{(t)} + \\vec{b}^{L,L+1} $$\n",
    "and\n",
    "$$ \\hat{\\vec{y}}^{(t)} = \\sigma_{L+1} \\left( \\vec{z}_{L+1}^{(t)} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations given for the forward propagation can seem a bit messy, so it is nice to have a more visual aid of what is going on. Here is a diagram of the complete RNN including the weights and biases relating the different nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward.svg\" width=\"720px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a weights and biases connected to a single arbitrary node. The green arrows represent input to the node, and the red arrows represent the output from the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward_node.svg\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the connections resulting in $\\vec{h}_l^{(t)}$ in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/activation.svg\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation in an RNN works by comparing the output of the network to some target output (just as in the regular neural network), and propagating backwards through both the layers and the *time sequence*. It is therefore commonly referred to as *backpropagation through time* (BPTT). We will now derive the necessary equations to perform BPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that we have propagated forward through the network, and have produced some output $\\hat{\\vec{y}}^{(t)}$. We want to compare this with some target output value $\\vec{y}^{(t)}$, and will do so through a cost function $C \\left(\\hat{\\vec{y}}, \\vec{y} \\right)$. We will denote the cost at a specific time step $t$ by $C^{(t)} = C^{(t)} \\left(\\hat{\\vec{y}}^{(t)}, \\vec{y}^{(t)} \\right)$, and the overall cost of the network as $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cost function at each time step, we want to compute the gradient with respect to each weight and bias, that is, we want to compute\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial W^{l_1 l_2}} \\; \\text{ and } \\; \\frac{\\partial C}{\\partial \\vec{b}^{l_1 l_2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do this one layer at a time, starting at the output layer, and propagating backwards through time in each layer. We assume that we know the gradient of the cost function with respect to the output $\\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}}$, and start by finding the gradient with respect to the output weights and biases $W^{L,L+1}$ and $\\vec{b}^{L,L+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to find the gradient with respect to $\\vec{z}_{L+1}^{(t)}$. The derivative of $C$ with respect to some element $z_{L+1, i}^{(t)}$ of the weighted sum is given by\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z_{L+1,i}^{(t)}} &= \\frac{\\partial C^{(t)}}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_j^{(t)}}  \\frac{\\partial \\hat{y}_j^{(t)}}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_j^{(t)}}  \\sigma_{L+1}^\\prime \\left( z_{L+1,i}^{(t)} \\right) \\delta_{ij}\n",
    "\\\\[4ex]\n",
    "&= \\frac{\\partial C^{(t)}}{\\partial \\hat{y}_i^{(t)}}  \\sigma_{L+1}^\\prime \\left( z_{L+1,i}^{(t)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\delta_{ij}$ is the Kronecker delta\n",
    "$\\delta_{ij} = \\begin{cases}\n",
    "0, & i \\neq j\\\\\n",
    "1, & i = j\n",
    "\\end{cases}$, and $\\sigma_{L+1}^\\prime$ denotes the derivative of the activation function, which we will assume to be known.\n",
    "we can write this expression more compactly in vector form as\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\vec{z}_{L+1}^{(t)}} = \\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}} \\odot \\sigma_{L+1}^\\prime \\left( \\vec{z}_{L+1}^{(t)} \\right),\n",
    "$$\n",
    "where $\\odot$ denotes the *Hadamard product*, an elementwise multiplication of two vectors/matrices of same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>**Note:**</u> Sometimes the derivatives are real numbers like $\\frac{\\partial C^{(t)}}{\\partial z_{L+1,i}^{(t)}}$, sometimes they are vectors such as $\\frac{\\partial C^{(t)}}{\\partial \\vec{z}_{L+1}^{(t)}}$, and sometimes they are matrices. I have not included any explicit notation to explain when they are what, but will assume that this is understood implicitly. A general rule would be to look at whether the expression contains indices like $i,j,k,\\ldots$ or not.\n",
    "\n",
    "<u>**Another note:**</u> There are a lot of indices to keep track of, so to make the notation simpler to follow I will try to follow these rules consistently:\n",
    "- $l$ = layer index (with $L$ being the final hidden layer). If I need several layer indices I will use $l_1,l_2,\\ldots$.\n",
    "- $(t)$ = time step index.\n",
    "- $i,j,k$ = vector/matrix elements.\n",
    "- $n$ = number of input features (length of $\\vec{X}$).\n",
    "- $m$ = number of output features (length of $\\hat{\\vec{y}}$).\n",
    "- $n_1,n_2,\\ldots$ = number of features in hidden layer number $1,2,\\ldots$.\n",
    "\n",
    "<u>**Third note:**</u> I will not always write the upper bound of summations explicitly, but will assume that this is understood implicitly. For instance, $\\sum_j W^{l-1,l}_{ij} h_{l-1,j}$ should be understood to mean $\\sum_{j=1}^{n_{l-1}} W^{l-1,l}_{ij} h_{l-1,j}$, such that it sums over all elements of $\\vec{h}_{l-1}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative with respect to the weighted sum will be used a lot during backpropagation, so we will give it its own notation\n",
    "$$ \\vec{\\delta}_{L+1}^{(t)} \\equiv \\frac{\\partial C^{(t)}}{\\partial \\vec{z}_{L+1}^{(t)}} = \\frac{\\partial C^{(t)}}{\\partial \\hat{\\vec{y}}^{(t)}} \\odot \\sigma_{L+1}^\\prime \\left( \\vec{z}_{L+1}^{(t)} \\right).$$\n",
    "$\\delta_{L+1}^{(t)}$ has one index downstairs (denoting layer), and one index upstairs in parentheses (denoting time step), so don't mix it up with the Kronecker delta $\\delta_{ij}$, which I will consistently write with two indices downstairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the delta we can find the cost gradient with respect to the output bias.\n",
    "Note that the same weights and biases occur several times in the RNN, so we have to sum over each contribution. The cost gradients with respect to the weights and biases in layer $l$ are denoted $\\frac{\\partial C}{\\partial W^{l-1,l}}$, $\\frac{\\partial C}{\\partial W^{ll}}$, $\\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}}$ and $\\frac{\\partial C}{\\partial \\vec{b}^{ll}}$, and we will denote the contribution at time step $t$ as $\\left(\\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)}$, $\\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)}$, $\\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)}$ and $\\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)}$ such that $\\frac{\\partial C}{\\partial W^{l-1,l}} = \\sum_t \\left( \\frac{\\partial C}{\\partial W^{l-1,l}}\\right)^{(t)}$ and so on.\n",
    "Using this notation, the gradient with respect to the output bias becomes\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial C}{\\partial b^{L,L+1}_i} \\right)^{(t)} &= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\frac{\\partial z_{L+1,j}^{(t)}}{\\partial b^{L,L+1}_i}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\frac{\\partial}{\\partial b^{L,L+1}_i} \\left( \\sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b^{L,L+1}_j \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_{j=1}^m \\frac{\\partial C}{\\partial z_{L+1,j}^{(t)}} \\delta_{ij}\n",
    "\\\\[4ex]\n",
    "&= \\frac{\\partial C}{\\partial z_{L+1,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\delta_{L+1,i}^{(t)}.\n",
    "\\end{align*}\n",
    "\n",
    "Thus on vector form we have\n",
    "$$ \\left( \\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} \\right)^{(t)} = \\vec{\\delta}_{L+1}^{(t)},$$\n",
    "and finally\n",
    "$$\\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} = \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{L,L+1}} \\right)^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the gradient with respect to the output weights\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial C}{W^{L,L+1}_{ij}} \\right)^{(t)} &= \\sum_{k_1=1}^m \\frac{\\partial C}{\\partial z_{L+1,k_1}^{(t)}} \\frac{\\partial z_{L+1,k_1}^{(t)}}{\\partial W^{L,L+1}_{ij}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_{k_1=1}^m \\delta_{L+1,k_1}^{(t)} \\frac{\\partial}{\\partial W^{L,L+1}_{ij}}\n",
    "\\left( \\sum_{k_2} W^{L,L+1}_{k_1 k_2} h_{L,k_2}^{(t)} + b^{L,L+1}_{k_1} \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_{k_1=1}^m \\delta_{L+1,k_1}^{(t)} \\sum_{k_2} h_{L,k_2}^{(t)} \\delta_{i k_1} \\delta_{j k_2}\n",
    "\\\\[4ex]\n",
    "&= \\delta_{L+1,i}^{(t)} h_{L,j}^{(t)}\n",
    "\\\\[4ex]\n",
    "&= \\left[ \\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T \\right]_{ij}.\n",
    "\\end{align*}\n",
    "\n",
    "Thus on vector form we have\n",
    "\n",
    "$$ \\left( \\frac{\\partial C}{W^{L,L+1}} \\right)^{(t)} = \\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T, $$\n",
    "and\n",
    "$$\\frac{\\partial C}{W^{L,L+1}} = \\sum_t \\left( \\frac{\\partial C}{W^{L,L+1}} \\right)^{(t)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we here have an outer product between two vectors, which results in a matrix:\n",
    "\n",
    "$$\n",
    "\\vec{\\delta}_{L+1}^{(t)} \\left(\\vec{h}_{L}^{(t)}\\right)^T\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\delta_{L+1,1}^{(t)} \\\\ \\vdots \\\\ \\delta_{L+1,m}^{(t)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "h_{L,1}^{(t)} & \\cdots & h_{L,n_L}^{(t)}\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "\\delta_{L+1,1}^{(t)} h_{L,1}^{(t)} & \\cdots & \\delta_{L+1,1}^{(t)} h_{L,n_L}^{(t)}\n",
    "\\\\\n",
    "\\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "\\delta_{L+1,m}^{(t)} h_{L,1}^{(t)} & \\cdots & \\delta_{L+1,m}^{(t)} h_{L,n_L}^{(t)}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to compute the gradient with respect to the output from the previous layer $\\frac{\\partial C}{\\partial \\vec{h}_L^{(t)}}$, in order to continue backpropagating through previous layers. We find this in much the same way as we found the other gradients above.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial h_{L,i}^{(t)}} &= \\sum_j \\frac{\\partial C}{z_{L+1,j}^{(t)}} \\frac{\\partial z_{L+1,j}^{(t)}}{\\partial h_{L,i}^{(t)}}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} \\frac{\\partial}{\\partial h_{L,i}^{(t)}} \\left( \\sum_k W^{L,L+1}_{jk} h_{L,k}^{(t)} + b_j^{L,L+1} \\right)\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} \\sum_k W^{L,L+1}_{jk} \\delta_{ik}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\delta_{L+1,j}^{(t)} W^{L,L+1}_{ji}\n",
    "\\\\[4ex]\n",
    "&= \\sum_j \\left[ \\left( W^{L,L+1} \\right)^T \\right]_{ij} \\delta_{L+1,j}^{(t)}\n",
    "\\\\[4ex]\n",
    "&= \\left[ \\left(W^{L,L+1} \\right)^T \\vec{\\delta}_{L+1}^{(t)} \\right]_i\n",
    "\\end{align*}\n",
    "\n",
    "And thus on vector form we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial \\vec{h}_L^{(t)}} = \\left( W^{L,L+1} \\right)^T \\vec{\\delta}_{L+1}^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a diagram showing the backpropagation through the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/backprop_output.svg\" width=720px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation through arbitrary node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some arbitrary node in the RNN with output $\\vec{h}_l^{(t)}$. Assume you know the total gradient of the cost with respect to this output from the two suceeding nodes\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} = \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{layer} + \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{time}.\n",
    "$$\n",
    "We now want to compute the gradients with respect to the weights and biases connecting the two previous nodes to this node, so that we can update these weights and biases when training the network, as well as the gradient with respect to the two previous nodes, so that we can continue backpropagation through the other nodes. The situation is illustrated in the diagram below. The blue arrows show the input gradient from the succeeding nodes, and the red arrows show the gradients we want to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/backprop_node.svg\" width=720px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary gradients are derived in the same way as for the output layer, so I will simply state the results here. We get the following set of equations for backpropagating through a general node in the RNN.\n",
    "\\begin{align}\n",
    "\\delta_l^{(t)} &= \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\odot \\sigma_l^\\prime \\left(\\vec{z}_l^{(t)} \\right)\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)} = \\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)} &= \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_{l-1}^{(t)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_l^{(t-1)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l-1}^{(t)}} &= \\left[ \\left( W^{l-1,l} \\right)^{(t)} \\right]^T \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l}^{(t-1)}} &= \\left[ \\left( W^{ll} \\right)^{(t-1)} \\right]^T \\delta_l^{(t)},\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{b}^{ll}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial W^{l-1,l}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial W^{ll}} &= \\sum_t \\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this method we can start with the nodes in the output layer, and propagate backwards. The necessary input to one node is the output from backpropagating through the previous node. Thus we can use the equations above recursively, layer by layer, to backpropagate through the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/backprop_steps.svg\" width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The RNN code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the mathematical framework, we can develop the code for the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building a recurrent neural network, we need to define some functions. We need activation functions, cost functions and a way to differentiate these. We also need gradient descent schedulers to update our weights and biases when backpropagating. These functions are defined in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to choose which activation function to use in different layers of the RNN. Here we define some activation functions that can be used by the network. If you have developed a regular fully connected neural network in FYS-STK4155 these functions will probably look very familiar, as they are pretty much copied from those lecture notes. The main difference is that I have used JAX instead of autograd for automatic differentiation. Due to the way JAX vectorizes, I have not gotten gradients with jax to work for softmax, but have included grad_softmax() as its own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def identity(X):\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + jnp.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return jnp.where(X > jnp.zeros(X.shape), jnp.ones(X.shape), jnp.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return np.exp(X) / (np.sum(np.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def grad_softmax(X):\n",
    "    f = softmax(X)\n",
    "    return f - f**2\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return jnp.where(X > jnp.zeros(X.shape), X, jnp.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return jnp.where(X > jnp.zeros(X.shape), X, delta * X)\n",
    "\n",
    "\n",
    "def tanh(X):\n",
    "    return jnp.tanh(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to implement the cost functions. We include three cost functions here. The ordinary least squares (OLS) is used for regression problems, and the logistic regression and cross-entropy are used for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CostOLS(target):\n",
    "\n",
    "    def func(X):\n",
    "        return (1.0 / target.shape[0]) * jnp.sum((target - X) ** 2)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostLogReg(target):\n",
    "\n",
    "    def func(X):\n",
    "\n",
    "        return -(1.0 / target.shape[0]) * jnp.sum(\n",
    "            (target * jnp.log(X + 10e-10)) + ((1 - target) * jnp.log(1 - X + 10e-10))\n",
    "        )\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostCrossEntropy(target):\n",
    "\n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * jnp.sum(target * jnp.log(X + 10e-10))\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we use JAX for automatic differentiation, which is done with the function *grad* in the JAX library. For *grad* to work on a function, it cannot use regular numpy, but must use *jax.numpy*, which is why we imported and used this when defining the activation and cost functions. JAX's numpy is only used for these functions, while we stick with regular numpy for everything else. \n",
    "\n",
    "The RELU and leaky RELU activation functions are not continuously differentiable, so we will handle these explicitly in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def derivate(func):\n",
    "    if func.__name__ == \"RELU\":\n",
    "\n",
    "        def func(X):\n",
    "            return jnp.where(X > 0, 1, 0)\n",
    "\n",
    "        return func\n",
    "\n",
    "    elif func.__name__ == \"LRELU\":\n",
    "\n",
    "        def func(X):\n",
    "            delta = 10e-4\n",
    "            return jnp.where(X > 0, 1, delta)\n",
    "\n",
    "        return func\n",
    "\n",
    "    else:\n",
    "        return grad(func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *grad* function is not, in itself, vectorized. This means that if we send in an array to a function that has been differentiated, JAX will treat this as a function with an array as input, not as a function treating each element individually. This is better understood with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $f(x)=x^2$, with derivative $f^\\prime (x)=2x$. With JAX we get this with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "df = grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works if we input a scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = 2.0\n",
    "print(df(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we try to input an array of values we get an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error message: Gradient only defined for scalar-output functions. Output had shape: (10,).\n"
     ]
    }
   ],
   "source": [
    "x = np.linspace(0, 3, 10)\n",
    "\n",
    "try:\n",
    "    print(df(x))\n",
    "except TypeError as msg:\n",
    "    print(f\"Error message: {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because JAX does not treat each element in $x$ individually. To get around this we can use the *vmap* function in the JAX library, which vectorizes the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        0.6666667 1.3333334 2.        2.6666667 3.3333333 4.\n",
      " 4.6666665 5.3333335 6.       ]\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap\n",
    "df = vmap(grad(f))\n",
    "x = np.linspace(0, 3, 10)\n",
    "print(df(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *vmap* only vectorizes along one dimension. So if the input array contains several axes, we have to apply *vmap* for each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.         0.23076923 0.46153846]\n",
      "  [0.6923077  0.9230769  1.1538461 ]\n",
      "  [1.3846154  1.6153846  1.8461539 ]]\n",
      "\n",
      " [[2.0769231  2.3076923  2.5384614 ]\n",
      "  [2.7692308  3.         3.2307692 ]\n",
      "  [3.4615386  3.6923077  3.9230769 ]]\n",
      "\n",
      " [[4.1538463  4.3846154  4.6153846 ]\n",
      "  [4.8461537  5.076923   5.3076925 ]\n",
      "  [5.5384617  5.769231   6.        ]]]\n"
     ]
    }
   ],
   "source": [
    "df = vmap(vmap(vmap(grad(f))))\n",
    "x = np.linspace(0,3, 27).reshape((3,3,3))\n",
    "print(df(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is tedious, yes, but as far as I know it is the only way to make JAX differentiate elementwise. If you find a simpler work-around, feel free to share with the professor or the group teachers so these notes can be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedulers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to be able to choose which method we want to use for gradient descent when training the RNN. This is done by using a scheduler defined below. These schedulers are identical to the ones used in FYS-STK4155."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    Abstract class for Schedulers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    # should be overwritten\n",
    "    def update_change(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # overwritten if needed\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Constant(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        return self.eta * gradient\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Momentum(Scheduler):\n",
    "    def __init__(self, eta: float, momentum: float):\n",
    "        super().__init__(eta)\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        self.change = self.momentum * self.change + self.eta * gradient\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Adagrad(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        return self.eta * gradient * G_t_inverse\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class AdagradMomentum(Scheduler):\n",
    "    def __init__(self, eta, momentum):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        self.change = self.change * self.momentum + self.eta * gradient * G_t_inverse\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class RMS_prop(Scheduler):\n",
    "    def __init__(self, eta, rho):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.second = 0.0\n",
    "\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "        self.second = self.rho * self.second + (1 - self.rho) * gradient * gradient\n",
    "        return self.eta * gradient / (np.sqrt(self.second + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.second = 0.0\n",
    "\n",
    "\n",
    "class Adam(Scheduler):\n",
    "    def __init__(self, eta, rho, rho2):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.rho2 = rho2\n",
    "        self.moment = 0\n",
    "        self.second = 0\n",
    "        self.n_epochs = 1\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "        self.moment = self.rho * self.moment + (1 - self.rho) * gradient\n",
    "        self.second = self.rho2 * self.second + (1 - self.rho2) * gradient * gradient\n",
    "\n",
    "        moment_corrected = self.moment / (1 - self.rho**self.n_epochs)\n",
    "        second_corrected = self.second / (1 - self.rho2**self.n_epochs)\n",
    "\n",
    "        return self.eta * moment_corrected / (np.sqrt(second_corrected + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_epochs += 1\n",
    "        self.moment = 0\n",
    "        self.second = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement the code for the RNN. The network will be object-oriented, consisting of the following classes:\n",
    "\n",
    "- *RNN*: The complete network, consisting of several *Layer* objects.\n",
    "- *Layer*: Abstract class containing information that is shared across the different types of layers. It is the parent class of the following:\n",
    "    - *InputLayer*: Layer containing the input to the network. Does not contain any weights and biases.\n",
    "    - *RNNLayer*: The recurrent layer consisting of nodes in sequence.\n",
    "    - *OutputLayer*: Recurrent layer for output. Similar to RNNLayer, but does not have any connections between the nodes, only to the nodes at the same time step in the previous layer.\n",
    "    - *DenseLayer*: Fully connected layer, used to switch from a recurrent network to a regular fully connected network. Especially used for non-sequential output, for instance in classification where you want to classify the entire sequence with a single output.\n",
    "- *Node*: Contains information about a single node. This is where all the math of forward- and backpropagation takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code we want to be able to input batches of values, so that we can feed forward (and backpropagate) many inputs at once. The input to our network will therefore be an array with three axes:\n",
    "\n",
    "- The input axis, separating the different inputs in the batch.\n",
    "- The sequence axis, separating the different time steps of the sequence.\n",
    "- The feature axis, separating the different features of the input (the vector elements).\n",
    "\n",
    "This input is fed forward through the network, thus each layer has the same three axes. Within the layers, we separate each time step into their own node, thus the nodes only have two axes: the input and feature axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the RNN class is dependent on the Layer classes, and the Layer classes are dependent on the Node class, we will build the network from the down up, starting with the Node class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Node class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Node class takes care of the math discussed in the sections [Forward propagation](#forward-propagation) and [Backpropagation through arbitrary node](#backpropagation-through-arbitrary-node). I restate the relevant equations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><u>Forward propagation:</u></p>\n",
    "\n",
    "\\begin{align*}\n",
    "\\left( \\vec{z}_l^{(t)} \\right)_{layer} &= W^{l-1,l} \\vec{h}_{l-1}^{(t)} + \\vec{b}^{l-1,l}\n",
    "\\\\[4ex]\n",
    "\\left( \\vec{z}_l^{(t)} \\right)_{time} &= W^{ll} \\vec{h}_{l}^{(t-1)} + \\vec{b}^{ll}\n",
    "\\\\[4ex]\n",
    "\\vec{z}_l^{(t)} &= \\left( \\vec{z}_l^{(t)} \\right)_{layer} + \\left( \\vec{z}_l^{(t)} \\right)_{time}\n",
    "\\\\[4ex]\n",
    "\\vec{h}_l^{(t)} &= \\sigma_l \\left( \\vec{z}_l^{(t)} \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center><u>Backpropagation:</u></p>\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} &= \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{layer} + \\left( \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\right)_\\text{time}\n",
    "\\\\[4ex]\n",
    "\\delta_l^{(t)} &= \\frac{\\partial C}{\\partial \\vec{h}_l^{(t)}} \\odot \\sigma_l^\\prime \\left(\\vec{z}_l^{(t)} \\right)\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial \\vec{b}^{l-1,l}} \\right)^{(t)} = \\left( \\frac{\\partial C}{\\partial \\vec{b}^{ll}} \\right)^{(t)} &= \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{l-1,l}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_{l-1}^{(t)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\left( \\frac{\\partial C}{\\partial W^{ll}} \\right)^{(t)} &= \\delta_l^{(t)} \\left( \\vec{h}_l^{(t-1)} \\right)^T\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l-1}^{(t)}} &= \\left[ \\left( W^{l-1,l} \\right)^{(t)} \\right]^T \\delta_l^{(t)}\n",
    "\\\\[4ex]\n",
    "\\frac{\\partial C}{\\partial \\vec{h}_{l}^{(t-1)}} &= \\left[ \\left( W^{ll} \\right)^{(t-1)} \\right]^T \\delta_l^{(t)}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<u>**Note:**</u> The input and output from a node are two-dimensional arrays, with different inputs along the first axis, and the features along the second axis. The equations we have derived for computing forward- and backpropagation assume that we are working with column vectors, but when performing matrix multiplication with these two-dimensional arrays we are in practice working with row vectors. This has to be taken into account in our code.\n",
    "\n",
    "Consider, for instance, multiplying some weight matrix $W$ with a vector $\\vec{h}$. The way we have derived the equations this will look like\n",
    "\n",
    "$$\n",
    "W\\vec{h} = \\begin{pmatrix}\n",
    "W_{11} & \\cdots & W_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "W_{m1} & \\cdots & W_{mn}\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "h_1 \\\\ \\vdots \\\\ h_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "In the code, however, this will look like\n",
    "\n",
    "$$\n",
    "\\vec{h} W = \\begin{pmatrix}\n",
    "h_{11} & \\cdots & h_{1n}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "h_{N1} & \\cdots & h_{Nn}\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "W_{11} & \\cdots & W_{1m}\\\\\n",
    "\\vdots & \\ddots & \\vdots\\\\\n",
    "W_{n1} & \\cdots & W_{nm}\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "where $N$ is the number of inputs. Note that $W$ and $\\vec{h}$ have switched places in the matrix multiplication, and that $W$ is essentially a transpose of the corresponding $W$ in the previous case (since we are multiplying the columns with the $\\vec{h}$-vectors, instead of the rows).\n",
    "\n",
    "This is not a big difference, but can be taken into account by reversing the order of some of the matrix multiplications. The full set of changes that must be done to account for this are listed here:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "W \\vec{h} \\to \\vec{h} W \\\\\n",
    "\\vec{\\delta} \\vec{h}^T \\to \\vec{h}^T \\vec{\\delta}\\\\\n",
    "W^T \\vec{\\delta} \\to \\vec{\\delta} W^T,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and are taken care of in the code below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable # Used for type hints of functions\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Single node in the RNN. Computes forward propagation through a single node, stores the output of as a vector (1D array) of length 'n_features'. This class also computes the backpropagation through the node, and stores all the relevant gradients.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in this node (the length of the vector).\n",
    "    act_func (Callable)\n",
    "        The activation function of this node.\n",
    "    {b/W}_{layer/time} (ndarray)\n",
    "        Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.\n",
    "    h_{layer/time/output} (ndarray)\n",
    "        The output of the node in the previous layer/time step, or the output of this node.\n",
    "    z_output (ndarray)\n",
    "        The weighted sum output of this node (the output before activation).\n",
    "    grad_{b/W/h}_{layer/time} (ndarray)\n",
    "        The gradient of the cost function with respect to the appropriate variable.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray] = identity,\n",
    "            W_layer: np.ndarray = None,\n",
    "            b_layer: np.ndarray = None,\n",
    "            W_time: np.ndarray = None,\n",
    "            b_time: np.ndarray = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for Node objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in this node (the length of the vector).\n",
    "        act_func (Callable)\n",
    "            The activation function of this node.\n",
    "        {b/W}_{layer/time} (ndarray)\n",
    "            Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.act_func = act_func\n",
    "        self.W_layer = W_layer\n",
    "        self.b_layer = b_layer\n",
    "        self.W_time = W_time\n",
    "        self.b_time = b_time\n",
    "\n",
    "        ## Values from feed_forward()\n",
    "        self.h_layer = None # h from previous layer\n",
    "        self.h_time = None # h from previous time step\n",
    "        self.z_output = None # z for this node\n",
    "        self.h_output = None # h from this node\n",
    "\n",
    "        ## Values from backpropagate()\n",
    "        self.grad_b_layer = None\n",
    "        self.grad_b_time = None\n",
    "        self.grad_W_layer = None\n",
    "        self.grad_W_time = None\n",
    "        self.grad_h_layer = None\n",
    "        self.grad_h_time = None\n",
    "    \n",
    "    def set_Wb(\n",
    "            self,\n",
    "            W_layer: np.ndarray,\n",
    "            b_layer: np.ndarray,\n",
    "            W_time: np.ndarray,\n",
    "            b_time: np.ndarray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets the weights and biases to specific values. Used by the layer classes to ensure all nodes have correct weights and biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        {b/W}_{layer/time} (ndarray)\n",
    "            Bias(b) or weight matrix (W) for computing the contribution to this node from the node in the previous layer/time step.\n",
    "        \"\"\"\n",
    "        self.W_layer = W_layer\n",
    "        self.b_layer = b_layer\n",
    "        self.W_time = W_time\n",
    "        self.b_time = b_time\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            h_layer: np.ndarray,\n",
    "            h_time: np.ndarray = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes the output of this node from the output of the nodes at the previous layer and time step, using the equations for forward propagation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_{layer/time/output} (ndarray)\n",
    "            The output of the node in the previous layer/time step.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h_output (ndarray)\n",
    "            The output of this node.\n",
    "        \"\"\"\n",
    "        ## Save h_layer and h_time for use in backpropagation\n",
    "        self.h_layer = h_layer\n",
    "        self.h_time = h_time\n",
    "\n",
    "        num_inputs = h_layer.shape[0]\n",
    "\n",
    "        ## Compute weighted sum z for this node.\n",
    "        z_layer = h_layer @ self.W_layer + self.b_layer # Dimension example: (100,5)@(5,7) + (7) = (100,7)\n",
    "\n",
    "        if h_time is None:\n",
    "            # This node is at the first time step, thus not receiving any input from previous time steps.\n",
    "            z_time = np.zeros((num_inputs, self.n_features))\n",
    "        else:\n",
    "            z_time = h_time @ self.W_time + self.b_time\n",
    "        \n",
    "        self.z_output = z_layer + z_time # Save the weighted sum in the node\n",
    "\n",
    "        ## Compute activation of the node\n",
    "        h_output = self.act_func(self.z_output)\n",
    "\n",
    "        self.h_output = h_output # Save the output in the node\n",
    "        return h_output # Return output\n",
    "        \n",
    "\n",
    "    def backpropagate(\n",
    "            self,\n",
    "            dC_layer: np.ndarray = None,\n",
    "            dC_time: np.ndarray = None,\n",
    "            lmbd: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs backpropagation through this node. Computes the gradient of the cost function with respect to the weights and biases of this layer (and stores these gradients so the weights and biases can be updated in the layer object this node belongs to), and the gradient with respect to the output of the nodes in the previous layer and time step (and stores these as well for further backpropagation through the network).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dC_{layer/time} (ndarray)\n",
    "            Contribution of cost gradient w.r.t. this node from node at next layer/time.\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        \"\"\"\n",
    "        n_batches = self.h_output.shape[0]\n",
    "        \n",
    "        ## Total gradient is the sum of the gradient from \"next\" layer and time\n",
    "        if dC_time is None:\n",
    "            # If this is the last node in the layer, the gradient is just the gradient from the next layer\n",
    "            dC = dC_layer\n",
    "        elif dC_layer is None:\n",
    "            # If the next layer has no node at this time step (because it is a SingleOutputLayer), use only dC_time\n",
    "            dC = dC_time\n",
    "        else:\n",
    "            dC = dC_layer + dC_time\n",
    "        \n",
    "        ## delta (gradient of cost w.r.t. z)\n",
    "        if self.act_func.__name__ == \"softmax\":\n",
    "            grad_act = grad_softmax(self.z_output)\n",
    "        else:\n",
    "            grad_act = vmap(vmap(derivate(self.act_func)))(self.z_output) # vmap is necessary for jax to vectorize gradient properly\n",
    "        \n",
    "        delta = grad_act * dC # Hadamard product, i.e., elementwise multiplication\n",
    "\n",
    "        ## Gradients w.r.t. bias\n",
    "        self.grad_b_layer = np.sum(delta, axis=0) / n_batches\n",
    "        self.grad_b_time = np.sum(delta, axis=0) / n_batches\n",
    "\n",
    "        ## Gradients w.r.t. weights\n",
    "        self.grad_W_layer = self.h_layer.T @ delta / n_batches\n",
    "        self.grad_W_layer = self.grad_W_layer + self.W_layer * lmbd # Regularization factor\n",
    "        \n",
    "        if self.h_time is None:\n",
    "            self.grad_W_time = None\n",
    "        else:\n",
    "            self.grad_W_time = self.h_time.T @ delta / n_batches\n",
    "            self.grad_W_time = self.grad_W_time + self.W_time * lmbd # Regularization factor\n",
    "\n",
    "        ## Gradients w.r.t. input from previous nodes\n",
    "        # Need to not transpose delta in order for matrices to match up correctly, since we have batches along rows, and features along columns\n",
    "        self.grad_h_layer = delta @ self.W_layer.T\n",
    "        if self.h_time is None:\n",
    "            self.grad_h_time = None\n",
    "        else:\n",
    "            self.grad_h_time = delta @ self.W_time.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Layer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN consists of several layers of various types. The Layer class is an abstract class keeping track of attributes and methods that are common for the different types of layers. We define this class for better organization of the different layers, but it should never be used by itself, for instance by creating a Layer object. You will notice that most of the Layer methods are only declared, not implemented. These methods will vary between layer types, and are implemented in the Layer's child classes, although the Layer class gives a nice overview of which methods to expect from its children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations # Necessary to create typing hint of Layer within the class Layer\n",
    "\n",
    "class Layer:\n",
    "    \"\"\"\n",
    "    Abstract class for layers. The attributes given here are the attributes that are common to all layers, but they will also contain other attributes in addition to these.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in the nodes of this layer (the length of the vectors).\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    nodes (list)\n",
    "        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.\n",
    "    n_nodes (int)\n",
    "        Number of nodes in the layer. Is updated when adding or removing nodes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor to be called from child classes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in the nodes of this layer (the length of the vectors).\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.seed = seed\n",
    "        self.nodes = []\n",
    "        self.n_nodes = 0\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset_schedulers(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update_weights_all_nodes(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def add_node(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def remove_nodes(self):\n",
    "        \"\"\"\n",
    "        Remove all the nodes created for this layer.\n",
    "\n",
    "        NOTE\n",
    "        ----\n",
    "        The weights and biases of the nodes are still stored in the layer, so we can easily\n",
    "        create new nodes. Removing the nodes is used to allow the sequence length to vary with\n",
    "        each call of feed_forward().\n",
    "        \"\"\"\n",
    "        self.nodes = []\n",
    "        self.n_nodes = 0\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            prev_layer: Layer\n",
    "    ):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backpropagate(\n",
    "            self,\n",
    "            next_layer: Layer,\n",
    "            lmbd: float\n",
    "    ):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The InputLayer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to feed forward through an RNN, we will send a 3-dimensional array of shape (batch size, sequence length, number of features) as input. We then want to feed forward through the first hidden layer by computing the output of each node separately.\n",
    "Each node should then get information from the node at the previous time step, and the node at the previous layer. The way we implement forward propagation in this notebook, we need the previous layer to contain one node for each time step, and feed forward through these nodes.\n",
    "To make sure this also works for the first hidden layer, we include the InputLayer class, which takes the 3-dimensional input array to the network, and stores the value at each time step as 2-dimensional arrays in separate nodes. For all subsequent layers, we can then send in the previous layer as input to the feed_forward() method, work with each node separately.\n",
    "\n",
    "Note that feed_forward() will take a layer class as input for all other layers, but since this is the first layer (with no preceding layer), it will instead take in the 3-dimensional input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    \"\"\"\n",
    "    The input layer of the RNN, used for storing the input to the RNN in nodes for easy forward propagation through subsequent layers.\n",
    "    This class does not contain any weights or biases, nor does it implement backpropagation as there are no parameters to update.\n",
    "    InputLayer is the only class that can function as the first layer in the RNN, and it should only be used for this purpose.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in the nodes of this layer (the length of the vectors).\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    nodes (list)\n",
    "        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.\n",
    "    n_nodes (int)\n",
    "        Number of nodes in the layer. Is updated when adding or removing nodes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for InputLayer objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in the nodes of this layer (the length of the vectors).\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        super().__init__(n_features, seed)\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"\n",
    "        Add a node. Weights and biases are set to None, and activation to identity by default,\n",
    "        as none of these are relevant for the input layer.\n",
    "        \"\"\"\n",
    "        new_node = Node(self.n_features) # Activation and weights are not used for the input layer\n",
    "        self.nodes.append(new_node)\n",
    "        self.n_nodes += 1\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            X: np.ndarray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through the layer. As this is the input layer, this amounts to adding nodes and setting the output of each node to the value of the input at the corresponding time step. Also checks that the number of features of the input is the same as the layer expects it to be.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X (ndarray)\n",
    "            Input to the RNN, and thus to this layer. X has shape (batch size, sequence length, number of features)\n",
    "        \n",
    "        NOTE\n",
    "        ----\n",
    "        Unlike the other layers, this layer takes a numpy array as input instead of a Layer, since it is the first layer.\n",
    "        \"\"\"\n",
    "        X_shape = X.shape\n",
    "        sequence_length = X_shape[1]\n",
    "        \n",
    "        if not self.n_features == X_shape[2]:\n",
    "            # Input must have the same number of features as defined by the layer\n",
    "            raise ValueError(f\"Expected the number of features in the input layer to be {self.n_features}, got {X_shape[2]}.\")\n",
    "        \n",
    "        # Add a node to the layer for each time step, and set output\n",
    "        self.remove_nodes()\n",
    "        for i in range(sequence_length):\n",
    "            self.add_node()\n",
    "            self.nodes[i].h_output = X[:,i,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RNNLayer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNNLayer class represents the recurrent layers in the RNN. It takes the value at each time step in the preceding layer and computes the output from each node in its own layer, propagating forward through each node one by one, taking into account both the connections between the layers and the time steps. The math behind forward- and backpropagation are already taken care of by the Node class, so the RNN class is actually quite simple.\n",
    "In forward propagation, it simply goes through each node, inserts the output from the nodes at the previous layer and time step, and lets the nodes themselves do the rest, including storing the results. In backpropagation it goes through each node in reversed order, inserts the gradient of the cost function with respect to their output, lets the nodes calculate the gradients with respect to weights and biases, and then uses these to update the weights and biases of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    " \n",
    "class RNNLayer(Layer):\n",
    "    \"\"\"\n",
    "    The recurrent layer of the RNN. Computes forward- and backpropagation through a recurrent layer by feeding forward, or backpropagating, through each\n",
    "    node of the layer one at a time in sequence.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in the nodes of this layer (the length of the vectors).\n",
    "    n_features_prev (int)\n",
    "        Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    nodes (list)\n",
    "        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.\n",
    "    n_nodes (int)\n",
    "        Number of nodes in the layer. Is updated when adding or removing nodes.\n",
    "    act_func (Callable)\n",
    "        The activation function of this layer.\n",
    "    {b/W}_{layer/time} (ndarray)\n",
    "        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer), \n",
    "        or between time steps within this layer (time).\n",
    "    {b/W}_{layer/time}_size (tuple)\n",
    "        The shape of b_layer/W_layer/b_time/W_time.\n",
    "    scheduler_{b/W}_{layer/time} (Scheduler)\n",
    "        The scheduler for updating b_layer/W_layer/b_time/W_time with gradient descent when backpropagating through this layer.\n",
    "    is_dense (bool)\n",
    "        Tells if this is a DenseLayer or not. It is set to False for this layer, as it is not the DenseLayer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            n_features_prev: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray],\n",
    "            scheduler: Scheduler,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for RNNLayer objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in the nodes of this layer (the length of the vectors).\n",
    "        n_features_prev (int)\n",
    "            Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "        act_func (Callable)\n",
    "            The activation function of this layer.\n",
    "        scheduler (Scheduler)\n",
    "            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        super().__init__(n_features, seed)\n",
    "\n",
    "        self.n_features_prev = n_features_prev\n",
    "        self.act_func = act_func\n",
    "\n",
    "        self.W_layer = None\n",
    "        self.b_layer = None\n",
    "        self.W_time = None\n",
    "        self.b_time = None\n",
    "        \n",
    "        self.W_layer_size = (self.n_features_prev, self.n_features)\n",
    "        self.b_layer_size = (1, self.n_features)\n",
    "        self.W_time_size = (self.n_features, self.n_features)\n",
    "        self.b_time_size = (1, self.n_features)\n",
    "\n",
    "        self.scheduler_W_layer = copy(scheduler)\n",
    "        self.scheduler_W_time = copy(scheduler)\n",
    "        self.scheduler_b_layer = copy(scheduler)\n",
    "        self.scheduler_b_time = copy(scheduler)\n",
    "\n",
    "        self.is_dense = False\n",
    "        \n",
    "        self.reset_weights()\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset weights and biases to random values from a normal distribution.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        self.W_layer = np.random.normal(size=self.W_layer_size)\n",
    "        self.b_layer = np.random.normal(size=self.b_layer_size) * 0.01\n",
    "        self.W_time = np.random.normal(size=self.W_time_size)\n",
    "        self.b_time = np.random.normal(size=self.b_time_size) * 0.01\n",
    "    \n",
    "    def reset_schedulers(self):\n",
    "        \"\"\"\n",
    "        Reset the schedulers of the layer.\n",
    "        \"\"\"\n",
    "        self.scheduler_W_layer.reset()\n",
    "        self.scheduler_b_layer.reset()\n",
    "        self.scheduler_W_time.reset()\n",
    "        self.scheduler_b_time.reset()\n",
    "\n",
    "    def update_weights_all_nodes(self):\n",
    "        \"\"\"\n",
    "        Update the weights and biases in all nodes of the layer.\n",
    "        \"\"\"\n",
    "        new_W_layer = self.W_layer\n",
    "        new_W_time = self.W_time\n",
    "        new_b_layer = self.b_layer\n",
    "        new_b_time = self.b_time\n",
    "        for node in self.nodes:\n",
    "            node.set_Wb(new_W_layer, new_b_layer, new_W_time, new_b_time)\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"\n",
    "        Add a node with the weights and biases specified by the layer.\n",
    "        \"\"\"\n",
    "        new_node = Node(self.n_features, self.act_func, self.W_layer, self.b_layer, self.W_time, self.b_time)\n",
    "        self.nodes.append(new_node)\n",
    "        self.n_nodes += 1\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            prev_layer: Layer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through this layer one node at a time. The results are stored in the nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_layer (Layer)\n",
    "            The preceding layer of the RNN.\n",
    "        \"\"\"\n",
    "        self.remove_nodes()\n",
    "        n_nodes_prev = prev_layer.n_nodes\n",
    "\n",
    "        for i in range(n_nodes_prev):\n",
    "            # Get output of node from previous layer\n",
    "            prev_layer_node = prev_layer.nodes[i]\n",
    "            h_layer = prev_layer_node.h_output\n",
    "            \n",
    "            # Get output of node from previous time step\n",
    "            if i == 0:\n",
    "                # No previous node if this is the first time step\n",
    "                h_time = None\n",
    "            else:\n",
    "                prev_time_node = self.nodes[i-1]\n",
    "                h_time = prev_time_node.h_output\n",
    "            \n",
    "            # Create and compute new node at this time step\n",
    "            self.add_node()\n",
    "            new_node = self.nodes[i]\n",
    "            new_node.feed_forward(h_layer, h_time)\n",
    "    \n",
    "    def backpropagate(\n",
    "            self,\n",
    "            next_layer: Layer,\n",
    "            lmbd: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backpropagate through the layer one node at a time. The results are stored in the nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        next_layer (Layer)\n",
    "            The subsequent layer of the RNN.\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        \"\"\"\n",
    "        ## Check if the next layer is a DenseLayer\n",
    "        next_is_dense = next_layer.is_dense\n",
    "\n",
    "        ## Go through all nodes, starting with the last\n",
    "        for i in range(self.n_nodes-1, -1, -1):\n",
    "            ## Gradient from node at next layer\n",
    "            if next_is_dense:\n",
    "                ## If next layer is DenseLayer, consider only grad_h_layer in the last node in this layer\n",
    "                if i == self.n_nodes-1:\n",
    "                    node_layer = next_layer.nodes[0]\n",
    "                    dC_layer = node_layer.grad_h_layer\n",
    "                else:\n",
    "                    dC_layer = None\n",
    "            else:\n",
    "                ## If next layer is not DenseLayer, get gradient from all nodes\n",
    "                node_layer = next_layer.nodes[i]\n",
    "                dC_layer = node_layer.grad_h_layer\n",
    "\n",
    "            ## Gradient from node at next time step (unless this is the last node)\n",
    "            if i == self.n_nodes-1:\n",
    "                dC_time = None\n",
    "            else:\n",
    "                node_time = self.nodes[i+1]\n",
    "                dC_time = node_time.grad_h_time\n",
    "            \n",
    "            ## Backpropagate through this node. Results are stored in the nodes\n",
    "            node = self.nodes[i]\n",
    "            node.backpropagate(dC_layer, dC_time, lmbd)\n",
    "\n",
    "            ## Update weights and biases\n",
    "            grad_W_layer = node.grad_W_layer / self.n_nodes\n",
    "            grad_W_time = node.grad_W_time\n",
    "            grad_b_layer = node.grad_b_layer / self.n_nodes\n",
    "            grad_b_time = node.grad_b_time\n",
    "\n",
    "            self.W_layer -= self.scheduler_W_layer.update_change(grad_W_layer)\n",
    "            if grad_W_time is not None:\n",
    "                grad_W_time = grad_W_time / (self.n_nodes - 1)\n",
    "                self.W_time -= self.scheduler_W_time.update_change(grad_W_time)\n",
    "            \n",
    "            self.b_layer -= self.scheduler_b_layer.update_change(grad_b_layer)\n",
    "            if grad_b_time is not None:\n",
    "                grad_b_time = grad_b_time / (self.n_nodes - 1)\n",
    "                self.b_time -= self.scheduler_b_time.update_change(grad_b_time)\n",
    "        \n",
    "        self.update_weights_all_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The OutputLayer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OutputLayer class gives the output of the RNN, with a value at each time step. It is very similar to the RNNLayer class, as they essentially do the same thing, but it only connects nodes at the previous layer to the corresponding output nodes, without any connections between ndoes at different time steps. Note also that, in the same way as *feed_forward()* in the InputLayer took a numpy array as input instead of a Layer object, the OutputLayer takes a numpy array as input to the *backpropagation()* method, since there is no subsequent layer to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(Layer):\n",
    "    \"\"\"\n",
    "    The output layer of the RNN, computing an output for each time step of the RNN. Very similar to RNNLayer, but has no connections between the nodes, only from the previous.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in the nodes of this layer (the length of the vectors).\n",
    "    n_features_prev (int)\n",
    "        Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    nodes (list)\n",
    "        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.\n",
    "    n_nodes (int)\n",
    "        Number of nodes in the layer. Is updated when adding or removing nodes.\n",
    "    act_func (Callable)\n",
    "        The activation function of this layer.\n",
    "    {b/W}_layer (ndarray)\n",
    "        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer).\n",
    "    {b/W}_layer_size (tuple)\n",
    "        The shape of b_layer/W_layer.\n",
    "    scheduler_{b/W}_layer (Scheduler)\n",
    "        The scheduler for updating b_layer/W_layer with gradient descent when backpropagating through this layer.\n",
    "    is_dense (bool)\n",
    "        Tells if this is a DenseLayer or not. It is set to False for this layer, as this is not a DenseLayer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            n_features_prev: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray],\n",
    "            scheduler: Scheduler,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for OutputLayer objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in the nodes of this layer (the length of the vectors).\n",
    "        n_features_prev (int)\n",
    "            Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "        act_func (Callable)\n",
    "            The activation function of this layer.\n",
    "        scheduler (Scheduler)\n",
    "            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        super().__init__(n_features, seed)\n",
    "        \n",
    "        self.n_features_prev = n_features_prev\n",
    "        self.act_func = act_func\n",
    "\n",
    "        self.W_layer = None\n",
    "        self.b_layer = None\n",
    "        \n",
    "        self.W_layer_size = (self.n_features_prev, self.n_features)\n",
    "        self.b_layer_size = (1, self.n_features)\n",
    "\n",
    "        self.scheduler_W_layer = copy(scheduler)\n",
    "        self.scheduler_b_layer = copy(scheduler)\n",
    "\n",
    "        self.is_dense = False\n",
    "        \n",
    "        self.reset_weights()\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset weights and biases to random values from a normal distribution.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        self.W_layer = np.random.normal(size=self.W_layer_size)\n",
    "        self.b_layer = np.random.normal(size=self.b_layer_size) * 0.01\n",
    "    \n",
    "    def reset_schedulers(self):\n",
    "        \"\"\"\n",
    "        Reset the schedulers of the layer.\n",
    "        \"\"\"\n",
    "        self.scheduler_W_layer.reset()\n",
    "        self.scheduler_b_layer.reset()\n",
    "\n",
    "    def update_weights_all_nodes(self):\n",
    "        \"\"\"\n",
    "        Update the weights and biases in all nodes of the layer.\n",
    "        \"\"\"\n",
    "        new_W_layer = self.W_layer\n",
    "        new_b_layer = self.b_layer\n",
    "        for node in self.nodes:\n",
    "            node.set_Wb(W_layer=new_W_layer, b_layer=new_b_layer)\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"\n",
    "        Add a node with the weights and biases specified by the layer.\n",
    "        \"\"\"\n",
    "        new_node = Node(self.n_features, self.act_func, self.W_layer, self.b_layer)\n",
    "        self.nodes.append(new_node)\n",
    "        self.n_nodes += 1\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            prev_layer: Layer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through this layer one node at a time. The results are stored in the nodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_layer (Layer)\n",
    "            The preceding layer of the RNN.\n",
    "        \"\"\"\n",
    "        self.remove_nodes()\n",
    "        n_nodes_prev = prev_layer.n_nodes\n",
    "\n",
    "        for i in range(n_nodes_prev):\n",
    "            # Get output of node from previous layer\n",
    "            prev_layer_node = prev_layer.nodes[i]\n",
    "            h_layer = prev_layer_node.h_output\n",
    "            \n",
    "            # Create and compute new node at this time step\n",
    "            self.add_node()\n",
    "            new_node = self.nodes[i]\n",
    "\n",
    "            # No info transfer between time steps in output layer\n",
    "            new_node.feed_forward(h_layer, None)\n",
    "            \n",
    "    \n",
    "    def backpropagate(\n",
    "            self,\n",
    "            dC: np.ndarray,\n",
    "            lmbd: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backpropagate through the layer one node at a time. The results are stored in the nodes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dC (ndarray)\n",
    "            Gradient of the cost function with respect to the output of the RNN. It has shape (batch size, sequence length, number of features).\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        \n",
    "        NOTE\n",
    "        ----\n",
    "        Unlike the other layers, this layer takes a numpy array as input instead of a Layer, since it is the last layer.\n",
    "        \"\"\"\n",
    "        ## Go through all nodes\n",
    "        for i in range(self.n_nodes):            \n",
    "            ## Backpropagate through this node. Results are stored in the nodes\n",
    "            node = self.nodes[i]\n",
    "            dC_layer = dC[:,i,:] # Treat dC as coming from a subsequent layer\n",
    "            node.backpropagate(dC_layer, None, lmbd) # No time gradient in the output layer\n",
    "\n",
    "            ## Update weights and biases (no time gradient in output layer)\n",
    "            grad_W_layer = node.grad_W_layer\n",
    "            grad_b_layer = node.grad_b_layer\n",
    "\n",
    "            self.W_layer -= self.scheduler_W_layer.update_change(grad_W_layer)\n",
    "            self.b_layer -= self.scheduler_b_layer.update_change(grad_b_layer)\n",
    "        \n",
    "        self.update_weights_all_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DenseLayer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have created the layers of a recurrent neural network with sequential input and sequential output, i.e., a network looking something like the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_seq_output.svg\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for situations where we want to predict some value at each input, such as solving differential equations. Other times, however, we want a single output for the entire input sequence, such as for sentiment analysis, where we want to determine if a sentence has a positive, negative or neutral tone. For such cases we still want to use an RNN with sequential input, but we don't want a sequential output. One way to accomplish this is to use a dense (fully connected) layer at the end of the network. We then take the last recurrent layer in the network, and add a connection from the last node to the dense layer. This would look like the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_dense_output.svg\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that we use a single node for the dense layer. This is because we have defined the nodes in this code to contain a single time step, but be vectors that represent the different features. Thus the connection from one node to another is really a dense connection between two \"layers\" (in the regular fully connected neural network sense). To convince yourself that the dense layer represented by a single node in the diagram is truly a dense *layer*, remember the following picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_dense_output_highlight.svg\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"\n",
    "    A dense layer used to go from a recurrent to a fully connected neural network. Contains a single node to feed forward and backpropagate through.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_features (int)\n",
    "        Number of features in the node of this layer (the length of the vector).\n",
    "    n_features_prev (int)\n",
    "        Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    nodes (list)\n",
    "        List containing all the nodes of the layer in sequence, such that nodes[0] is the node at the first time step, nodes[1] at the second time step, and so on.\n",
    "    n_nodes (int)\n",
    "        Number of nodes in the layer. Is updated when adding or removing nodes.\n",
    "    act_func (Callable)\n",
    "        The activation function of this layer.\n",
    "    {b/W}_layer (ndarray)\n",
    "        Bias (b) or weight matrix (W) for computing the forward propagation from nodes at the previous layer to this one (layer).\n",
    "    {b/W}_layer_size (tuple)\n",
    "        The shape of b_layer/W_layer.\n",
    "    scheduler_{b/W}_layer (Scheduler)\n",
    "        The scheduler for updating b_layer/W_layer with gradient descent when backpropagating through this layer.\n",
    "    is_dense (bool)\n",
    "        Tells if this is a DenseLayer or not. It is set to True for this layer, as this is a DenseLayer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            n_features_prev: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray],\n",
    "            scheduler: Scheduler,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for OutputLayer objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in the node of this layer (the length of the vector).\n",
    "        n_features_prev (int)\n",
    "            Number of features in the nodes of the preceding layer (the length of the vectors).\n",
    "        act_func (Callable)\n",
    "            The activation function of this layer.\n",
    "        scheduler (Scheduler)\n",
    "            The scheduler to use for updating weights and biases with gradient descent when backpropagating through this layer.\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        super().__init__(n_features, seed)\n",
    "\n",
    "        self.n_features_prev = n_features_prev\n",
    "        self.act_func = act_func\n",
    "\n",
    "        self.nodes = []\n",
    "        self.n_nodes = 0\n",
    "\n",
    "        self.W_layer = None\n",
    "        self.b_layer = None\n",
    "        \n",
    "        self.W_layer_size = (self.n_features_prev, self.n_features)\n",
    "        self.b_layer_size = (1, self.n_features)\n",
    "\n",
    "        self.scheduler_W_layer = copy(scheduler)\n",
    "        self.scheduler_b_layer = copy(scheduler)\n",
    "\n",
    "        self.is_dense = True\n",
    "        \n",
    "        self.reset_weights()\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset weights and biases to random values from a normal distribution.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        self.W_layer = np.random.normal(size=self.W_layer_size)\n",
    "        self.b_layer = np.random.normal(size=self.b_layer_size) * 0.01\n",
    "    \n",
    "    def reset_schedulers(self):\n",
    "        \"\"\"\n",
    "        Reset the schedulers of the layer.\n",
    "        \"\"\"\n",
    "        self.scheduler_W_layer.reset()\n",
    "        self.scheduler_b_layer.reset()\n",
    "\n",
    "    def update_weights_all_nodes(self):\n",
    "        \"\"\"\n",
    "        Update the weights and biases in the node.\n",
    "        \"\"\"\n",
    "        self.nodes[0].set_Wb(self.W_layer, self.b_layer)\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"\n",
    "        Add a node with the weights and biases specified by the layer.\n",
    "        \"\"\"\n",
    "        new_node = Node(self.n_features, self.act_func, self.W_layer, self.b_layer)\n",
    "        self.nodes.append(new_node)\n",
    "        self.n_nodes += 1\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            prev_layer: Layer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through this layer. The result is stored in the node.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_layer (Layer)\n",
    "            The preceding layer of the RNN.\n",
    "        \"\"\"\n",
    "        ## Get output from last node of previous layer\n",
    "        prev_node = prev_layer.nodes[-1]\n",
    "        h_layer = prev_node.h_output\n",
    "\n",
    "        self.remove_nodes()\n",
    "        self.add_node()\n",
    "        new_node = self.nodes[0]\n",
    "        output = new_node.feed_forward(h_layer)\n",
    "\n",
    "        return output\n",
    "            \n",
    "    \n",
    "    def backpropagate(\n",
    "            self,\n",
    "            next_layer_or_dC: Layer | np.ndarray,\n",
    "            lmbd: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backpropagate through the layer. The results are stored in the node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        next_layer_or_dC (Layer | ndarray)\n",
    "            This variable should be one of the two, depending on the type of input, whether it is a Layer object or a numpy array:\n",
    "            - Layer: The subsequent layer of the network. Used if this is not the last layer in the network.\n",
    "            - Numpy array: The gradient of the cost function with respect to the output of the network.\n",
    "              It has shape (batch size, number of features). Used if this is the last layer (output) of the network.\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        \"\"\"\n",
    "        ## Get dC from next_layer_or_dC.\n",
    "        if isinstance(next_layer_or_dC, Layer):\n",
    "            # If next_layer_or_dC is a Layer, extract dC from last node.\n",
    "            dC = next_layer_or_dC.nodes[0].grad_h_layer\n",
    "        else:\n",
    "            # If next_layer_or_dC is not a layer, it is the cost gradient.\n",
    "            dC = next_layer_or_dC\n",
    "        \n",
    "        ## Backpropagate through the node\n",
    "        node = self.nodes[0]\n",
    "        node.backpropagate(dC_layer=dC, dC_time=None, lmbd=lmbd)\n",
    "\n",
    "        ## Update weights and biases\n",
    "        grad_W_layer = node.grad_W_layer\n",
    "        grad_b_layer = node.grad_b_layer\n",
    "\n",
    "        self.W_layer -= self.scheduler_W_layer.update_change(grad_W_layer)\n",
    "        self.b_layer -= self.scheduler_b_layer.update_change(grad_b_layer)\n",
    "\n",
    "        self.update_weights_all_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created all the different layer classes we need for the RNN, we can create the network itself. The RNN class is used to organize the layers in order to train and use the RNN.\n",
    "\n",
    "Note that the class has two options for predicting results from data. The method *feed_forward()* propagates the input forward and returns the output in the same way as we have looked at so far. The *predict()* method starts by running *feed_forward()*, then uses argmax to set one element in the output to $1$ and the rest to $0$. In other words, *feed_forward()* is used for regression problems, while *predict()* is used for classification.\n",
    "\n",
    "In addition to this, we also have a method named *extrapolate()*, which can be used after having run *feed_forward()*. This method takes the output at the last time step, and uses it as input for new time steps, extending the sequence. This is shown in the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_extrapolate.svg\" width=720px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "class RNN:\n",
    "    \"\"\"\n",
    "    The recurrent neural network. Builds the network by adding layers, trains it on data and predicts output of new data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layers (list)\n",
    "        List containing all the layers of the RNN in sequence, such that layers[0] is the first layer, nodes[1] is the second layer, and so on.\n",
    "    n_layers (int)\n",
    "        Number of layers in the RNN. Is updated when adding layers.\n",
    "    cost_func (Callable)\n",
    "        Function which takes in the target array and returns a new function. The new function takes in the output from the network,\n",
    "        and returns the cost of that output when compared to the target.\n",
    "    scheduler (Scheduler)\n",
    "        The scheduler to use for updating weights and biases with gradient descent when backpropagating through the network.\n",
    "    seed (int)\n",
    "        Seed for random number generating with numpy.\n",
    "    single_output (bool)\n",
    "        False if the network has a sequential output (using OutputLayer), and True if the network has a single output (using DenseLayer).\n",
    "    n_features_output (int)\n",
    "        Number of features in the output layer.\n",
    "    output (ndarray)\n",
    "        The output from the network after computing forward propagation.\n",
    "    predicted (ndarray)\n",
    "        The prediction from the network on classification problems.\n",
    "    output_extra (ndarray)\n",
    "        Output from extrapolating the time sequence beyond the length of the input.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            cost_func: Callable[\n",
    "                [np.ndarray], # Takes in the target output (array)\n",
    "                Callable[[np.ndarray], np.ndarray] # Returns a function (the cost function)\n",
    "            ],\n",
    "            scheduler: Scheduler,\n",
    "            seed: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for RNN objects.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cost_func (Callable)\n",
    "            Function which takes in the target array and returns a new function. The new function takes in the output from the network,\n",
    "            and returns the cost of that output when compared to the target.\n",
    "        scheduler (Scheduler)\n",
    "            The scheduler to use for updating weights and biases with gradient descent when backpropagating through the network.\n",
    "        seed (int)\n",
    "            Seed for random number generating with numpy.\n",
    "        \"\"\"\n",
    "        self.layers = [] # List of layers\n",
    "        self.n_layers = 0\n",
    "\n",
    "        self.cost_func = cost_func\n",
    "        self.scheduler = scheduler\n",
    "        self.seed = seed\n",
    "        self.single_output = None # Boolean. Will update this when adding OutputLayer or DenseLayer output\n",
    "\n",
    "        self.n_features_output = None\n",
    "        self.output = None\n",
    "        self.predicted = None\n",
    "        self.output_extra = None\n",
    "    \n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Reset weights and biases in all layers to random values from a normal distribution.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.reset_weights()\n",
    "    \n",
    "    def reset_schedulers(self):\n",
    "        \"\"\"\n",
    "        Reset the schedulers of the RNN.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.reset_schedulers()\n",
    "\n",
    "    def feed_forward(\n",
    "            self,\n",
    "            X: np.ndarray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through the RNN one layer at a time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X (ndarray)\n",
    "            Input to the RNN, with shape (batch size, sequence length, number of features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output (ndarray)\n",
    "            Output from the RNN. If the network produces sequential output, it has the same shape as the input X. If the network produces a single output, it has the shape (batch size, number of features), that is, the same shape as for sequential output, but without the sequence axis.\n",
    "        \"\"\"\n",
    "        X_shape = X.shape\n",
    "\n",
    "        ## Initialize output\n",
    "        n_batches = X_shape[0]\n",
    "        sequence_length = X_shape[1]\n",
    "        n_features_output = self.n_features_output\n",
    "        \n",
    "        if self.single_output:\n",
    "            output_shape = (n_batches, n_features_output)\n",
    "            output = np.zeros(output_shape)\n",
    "        else:\n",
    "            output_shape = (n_batches, sequence_length, n_features_output)\n",
    "            output = np.zeros(output_shape)\n",
    "            \n",
    "        ## Feed forward through all layers\n",
    "        self.layers[0].feed_forward(X)\n",
    "        for i in range(1, self.n_layers):\n",
    "            layer = self.layers[i]\n",
    "            prev_layer = self.layers[i-1]\n",
    "            layer.feed_forward(prev_layer)\n",
    "        \n",
    "        ## Get output from last layer\n",
    "        output_layer = layer\n",
    "        if output_layer.is_dense:\n",
    "            node = output_layer.nodes[0]\n",
    "            output = node.h_output\n",
    "        else:\n",
    "            for i in range(output_layer.n_nodes):\n",
    "                node = output_layer.nodes[i]\n",
    "                output[:,i,:] = node.h_output\n",
    "        \n",
    "        ## Store and return output\n",
    "        self.output = output\n",
    "        return self.output\n",
    "    \n",
    "    def predict(\n",
    "            self,\n",
    "            X: np.ndarray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Feed forward through the RNN, and use argmax to classify the results.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X (ndarray)\n",
    "            Input to the RNN, with shape (batch size, sequence length, number of features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        predicted (ndarray)\n",
    "            Predicted classification output. If the network produces sequential output, it has the same shape as the input X. If the network produces a single output, it has the shape (batch size, number of features), that is, the same shape as for sequential output, but without the sequence axis.\n",
    "        \"\"\"\n",
    "        ## Initialize predicted array\n",
    "        output = self.feed_forward(X)\n",
    "        predicted = np.zeros_like(output)\n",
    "\n",
    "        ## Find maximum values at the feature axis, and set predicted to 1 at those indices\n",
    "        ind_batch = np.arange(predicted.shape[0])\n",
    "        if self.single_output:\n",
    "            ind_max = np.argmax(output, axis=1)\n",
    "\n",
    "            # Set predicted to 1 at the maximum values\n",
    "            predicted[ind_batch, ind_max] = 1\n",
    "        else:\n",
    "            ind_seq = np.arange(output.shape[1])\n",
    "            ind_max = np.argmax(output, axis=2)\n",
    "            ind_seq, ind_batch = np.meshgrid(ind_seq, ind_batch)\n",
    "            \n",
    "            # Set predicted to 1 at the maximum values\n",
    "            predicted[ind_batch, ind_seq, ind_max] = 1\n",
    "        \n",
    "        self.predicted = predicted\n",
    "        return self.predicted\n",
    "\n",
    "\n",
    "    def extrapolate(\n",
    "            self,\n",
    "            length: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Extrapolate data by continuing the sequence from the output of the last layer, with the output of the previous time step as input for the new time step\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        length (int)\n",
    "            Number of time steps to extrapolate.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output_extra (ndarray)\n",
    "            Extrapolated output from the RNN. It has the shape (number of batches, extrapolation length, number of features).\n",
    "        \"\"\"\n",
    "        ## Initialize extrapolated output\n",
    "        output = self.output\n",
    "        n_batches = output.shape[0]\n",
    "        n_features = self.n_features_output\n",
    "        output_extra_shape = (n_batches, length, n_features)\n",
    "        output_extra = np.zeros(output_extra_shape)\n",
    "\n",
    "        ## Extrapolate\n",
    "        y_prev = output[:,0,:] # Output from last layer at last time step\n",
    "\n",
    "        for i in range(length):\n",
    "            h_layer = y_prev # h_layer at first layer = input = previous output\n",
    "            for l in range(1, self.n_layers):\n",
    "                layer = self.layers[l]\n",
    "                node_prev = layer.nodes[-1] # Node at previous time step\n",
    "                h_time = node_prev.h_output\n",
    "                # print(h_layer, h_time)\n",
    "                layer.add_node()\n",
    "                node = layer.nodes[-1] # Current node\n",
    "                if l == self.n_layers-1:\n",
    "                    # If this is the output layer (last layer), set h_time to None\n",
    "                    h_time = None\n",
    "                node.feed_forward(h_layer, h_time)\n",
    "                h_layer = node.h_output # Update h_layer\n",
    "            y_prev = h_layer\n",
    "            output_extra[:,i,:] = y_prev\n",
    "        \n",
    "        self.output_extra = output_extra\n",
    "        return self.output_extra\n",
    "\n",
    "    \n",
    "    def backpropagate(\n",
    "            self,\n",
    "            output: np.ndarray,\n",
    "            target: np.ndarray,\n",
    "            lmbd: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backpropagate through the RNN one layer at a time, and update all the weights and biases.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output (ndarray)\n",
    "            The output we get from forward propagation, used to compare with the target values.\n",
    "        target (ndarray)\n",
    "            The target output that we want to compare our results to.\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        \"\"\"\n",
    "        ## Find gradient of cost function\n",
    "        grad_cost = derivate(self.cost_func(target))\n",
    "        dC = grad_cost(output)\n",
    "\n",
    "        ## Backpropagate through all layers\n",
    "        self.layers[-1].backpropagate(dC, lmbd=lmbd)\n",
    "        for i in range(self.n_layers-2, 0, -1):\n",
    "            layer = self.layers[i]\n",
    "            next_layer = self.layers[i+1]\n",
    "            layer.backpropagate(next_layer, lmbd)\n",
    "        \n",
    "    def train(\n",
    "            self,\n",
    "            X_train: np.ndarray,\n",
    "            t_train: np.ndarray,\n",
    "            X_val: np.ndarray = None,\n",
    "            t_val: np.ndarray = None,\n",
    "            epochs: int = 100,\n",
    "            batches: int = 1,\n",
    "            lmbd: float = 0.01,\n",
    "            store_output: np.ndarray = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the RNN on data, tuning the weights and biases such that the network can make predictions on new unseen data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train (ndarray)\n",
    "            Input data to the network during training, with shape (batch size, sequence length, number of features).\n",
    "        t_train (ndarray)\n",
    "            Target output corresponding to the training input. Same shape as X_train if the output is sequential, (batch size, number of features) if not.\n",
    "        X_val (ndarray)\n",
    "            Optional. Validation data to see how the RNN performs on unseen data. Not used for training, only for calculating scores.\n",
    "        t_val (ndarray)\n",
    "            Optional. Target output corresponding to the validation data.\n",
    "        epochs (int)\n",
    "            Number of epochs to train for.\n",
    "        batches (int)\n",
    "            Number of batches to split data into at each epoch.\n",
    "        lmbd (float)\n",
    "            Regularization parameter for finding the cost gradient with respect to the weights.\n",
    "        store_output (bool)\n",
    "            Whether to store output from each epoch (if you want to see how the output evolves as the RNN trains).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scores (dict)\n",
    "            The different scores from the training stored as a dictionary, with the following keys:\n",
    "            - \"train_error\" : The error of the training data for each epoch.\n",
    "            - \"train_accuracy\" : The accuracy of the training data for each epoch.\n",
    "            - \"val_error\" : The error of the validation data for each epoch.\n",
    "            - \"val_accuracy\" : The accuracy of the validation data for each epoch.\n",
    "            - \"y_train_history\" : The output from the training data for each epoch.\n",
    "            - \"y_val_history\" : The output from the validation data for each epoch.\n",
    "        \"\"\"\n",
    "        self.reset_weights() # Reset weights for new training\n",
    "        batch_size = X_train.shape[0] // batches\n",
    "\n",
    "        # Initialize arrays for scores\n",
    "        train_cost = self.cost_func(t_train)\n",
    "        train_error = np.zeros(epochs)\n",
    "        train_accuracy = np.zeros(epochs)\n",
    "\n",
    "        if X_val is not None:\n",
    "            val_cost = self.cost_func(t_val)\n",
    "            val_error = np.zeros(epochs)\n",
    "            val_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        ## Initialize arrays for output history if this should be stored\n",
    "        if store_output:\n",
    "            n_batches_train = X_train.shape[0]\n",
    "            seq_length_train = X_train.shape[1]\n",
    "            n_features_output = self.n_features_output\n",
    "\n",
    "            if self.single_output:\n",
    "                y_train_shape = (epochs, n_batches_train, n_features_output)\n",
    "            else:\n",
    "                y_train_shape = (epochs, n_batches_train, seq_length_train, n_features_output)\n",
    "            \n",
    "            y_train_history = np.zeros(shape=y_train_shape)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                n_batches_val = X_val.shape[0]\n",
    "                seq_length_val = X_val.shape[1]\n",
    "\n",
    "                if self.single_output:\n",
    "                    y_val_shape = (epochs, n_batches_val, n_features_output)\n",
    "                else:\n",
    "                    y_val_shape = (epochs, n_batches_val, seq_length_val, n_features_output)\n",
    "                \n",
    "                y_val_history = np.zeros(shape=y_val_shape)\n",
    "        \n",
    "        # Resample X and t\n",
    "        X_train, t_train = resample(X_train, t_train, replace=False)\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print(\"EPOCH: \" + str(e+1) + \"/\" + str(epochs), end=\"\\r\")\n",
    "            for b in range(batches):\n",
    "                ## Extract a smaller batch from the training data\n",
    "                if b == batches - 1:\n",
    "                    # If this is the last batch, include all remaining elements\n",
    "                    X_batch = X_train[b*batch_size :]\n",
    "                    t_batch = t_train[b*batch_size :]\n",
    "                else:\n",
    "                    X_batch = X_train[b*batch_size : (b+1)*batch_size]\n",
    "                    t_batch = t_train[b*batch_size : (b+1)*batch_size]\n",
    "                \n",
    "                ## Train the network on this batch with gradient descent\n",
    "                y_batch = self.feed_forward(X_batch)\n",
    "                self.backpropagate(y_batch, t_batch, lmbd)\n",
    "            \n",
    "            self.reset_schedulers()\n",
    "\n",
    "            ## Compute scores for this epoch\n",
    "            y_train = self.feed_forward(X_train)\n",
    "            pred_train = self.predict(X_train)\n",
    "\n",
    "            train_error[e] = train_cost(y_train)\n",
    "            \n",
    "            train_acc_arr = np.all(pred_train == t_train, axis=-1)\n",
    "            train_accuracy[e] = np.mean(train_acc_arr)\n",
    "\n",
    "            if X_val is not None:\n",
    "                y_val = self.feed_forward(X_val)\n",
    "                pred_val = self.predict(X_val)\n",
    "                \n",
    "                val_error[e] = val_cost(y_val)\n",
    "            \n",
    "                val_acc_arr = np.all(pred_val == t_val, axis=-1)\n",
    "                val_accuracy[e] = np.mean(val_acc_arr)\n",
    "            \n",
    "            if store_output:\n",
    "                y_train_history[e] = y_train\n",
    "                if X_val is not None:\n",
    "                    y_val_history[e] = y_val\n",
    "        \n",
    "        ## Create a dictionary for the scores, and return it\n",
    "        scores = {\"train_error\": train_error, \"train_accuracy\": train_accuracy}\n",
    "        if X_val is not None:\n",
    "            scores[\"val_error\"] = val_error\n",
    "            scores[\"val_accuracy\"] = val_accuracy\n",
    "        \n",
    "        if store_output:\n",
    "            scores[\"y_train_history\"] = y_train_history\n",
    "            if X_val is not None:\n",
    "                scores[\"y_val_history\"] = y_val_history\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def add_InputLayer(\n",
    "            self,\n",
    "            n_features: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adds an InputLayer to the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features of the input.\n",
    "        \"\"\"\n",
    "        layer = InputLayer(n_features, self.seed)\n",
    "        self._add_layer(layer)\n",
    "    \n",
    "    def add_RNNLayer(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adds an RNNLayer to the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in this layer.\n",
    "        act_func (Callable)\n",
    "            The activation function to use for this layer.\n",
    "        \"\"\"\n",
    "        scheduler = copy(self.scheduler)\n",
    "        prev_layer = self.layers[-1]\n",
    "        n_features_prev = prev_layer.n_features\n",
    "        layer = RNNLayer(n_features, n_features_prev, act_func, scheduler, self.seed)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def add_OutputLayer(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adds an OutputLayer to the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in this layer.\n",
    "        act_func (Callable)\n",
    "            The activation function to use for this layer.\n",
    "        \"\"\"\n",
    "        scheduler = copy(self.scheduler)\n",
    "        prev_layer = self.layers[-1]\n",
    "        n_features_prev = prev_layer.n_features\n",
    "        layer = OutputLayer(n_features, n_features_prev, act_func, scheduler, self.seed)\n",
    "        self._add_layer(layer)\n",
    "        \n",
    "        self.single_output = False\n",
    "        self.n_features_output = n_features\n",
    "    \n",
    "    def add_DenseLayer(\n",
    "            self,\n",
    "            n_features: int,\n",
    "            act_func: Callable[[np.ndarray], np.ndarray],\n",
    "            is_last_layer: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adds a DenseLayer to the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features (int)\n",
    "            Number of features in this layer.\n",
    "        act_func (Callable)\n",
    "            The activation function to use for this layer.\n",
    "        is_last_layer (bool)\n",
    "            True if this is the last layer of the network (the output layer). False if not.\n",
    "        \"\"\"\n",
    "        scheduler = copy(self.scheduler)\n",
    "        prev_layer = self.layers[-1]\n",
    "        n_features_prev = prev_layer.n_features\n",
    "        layer = DenseLayer(n_features, n_features_prev, act_func, scheduler, self.seed)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "        if is_last_layer:\n",
    "            self.single_output = True\n",
    "            self.n_features_output = n_features\n",
    "\n",
    "    def _add_layer(\n",
    "            self,\n",
    "            layer: Layer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Adds a layer to the RNN by appending it to *layers*, and increases *n_layers* by one.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer (Layer)\n",
    "            The layer to add to the RNN.\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        self.n_layers += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
