{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network (RNN) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a recurrent neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN), as opposed to a regular fully connected neural network (FCNN), has layers that are connected to themselves.\n",
    "\n",
    "The difference might be clearer by first looking at an FCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/fcnn.svg\" width=\"40%\" alt=\"FCNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an FCNN there are no connections between nodes in a single layer. For instance, $h_1^1$ is not connected to $h_2^1$. In addition, the input and output are always of a fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an RNN, however, this is no longer the case. Nodes in the hidden layers are connected to themselves, represented by the curved lines in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the output $\\vec{h}$ from the hidden layer is fed back into the hidden layer. This recurrence makes RNNs useful when working with sequential data, as we can have input of variable length. This is more clear if we unfold the recurrent part of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/rnn_unfold.svg\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematics of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider some sequential input $X$ with $n$ features. Note that $X$ here is an array with two axes, since it contains $n$ features at each time step in the sequence. We will denote the input at a specific time step $t$ as\n",
    "$$\\vec{X}^{(t)} = \\begin{pmatrix}\n",
    "X^{(t)}_1 \\\\ \\vdots \\\\ X^{(t)}_n\n",
    "\\end{pmatrix},$$\n",
    "which is then an $n$-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an RNN with $L$ hidden layers, and an output layer with $m$ features. We will denote the output of the $l$'th hidden layer at time step $t$ as\n",
    "$$\\vec{h}_l^{(t)} = \\begin{pmatrix}\n",
    "h_{l, 1}^{(t)} \\\\ \\vdots \\\\ h_{l, n_l}^{(t)}\n",
    "\\end{pmatrix},$$\n",
    "with $n_l$ being the number of features in the $l$'th hidden layer. The output of the RNN at time step $t$ is denoted\n",
    "$$\\hat{\\vec{y}}^{(t)} = \\begin{pmatrix}\n",
    "\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_m,\n",
    "\\end{pmatrix}$$\n",
    "where the hat is there to distinguish the RNN output $\\hat{\\vec{y}}^{(t)}$ from the target value, which is denoted $\\vec{y}^{(t)}$.\n",
    "The RNN will then look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/large_rnn.svg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/feed_forward.svg\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
